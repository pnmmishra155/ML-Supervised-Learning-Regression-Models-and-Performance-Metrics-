{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment Supervised Learning: Regression Models and Performance Metrics"
      ],
      "metadata": {
        "id": "Ewronn85_kEn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 1 : What is Simple Linear Regression (SLR)? Explain its purpose.\n",
        "Answer:\n",
        "\n",
        "Simple Linear Regression (SLR) is a type of regression analysis used to establish a relationship between one independent variable (X) and one dependent variable (Y). It assumes that the relationship is linear, meaning the change in Y is proportional to the change in X. The model can be represented graphically as a straight line through data points, called the regression line.\n",
        "\n",
        "###Purpose:\n",
        "\n",
        "- Prediction: SLR allows us to predict future values of Y for given values of X.\n",
        "\n",
        "- Relationship analysis: It quantifies the strength and direction of the relationship. A positive slope indicates that Y increases as X increases; a negative slope indicates the opposite.\n",
        "\n",
        "- Decision-making: Organizations use it for forecasting and planning, e.g., predicting sales based on advertising expenditure.\n",
        "\n",
        "###Example:\n",
        " A company wants to predict revenue (Y) based on advertising budget (X). By fitting an SLR model, the company can estimate how revenue changes for each additional dollar spent on advertising"
      ],
      "metadata": {
        "id": "efS2Ng4e_vaI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 2: What are the key assumptions of Simple Linear Regression?\n",
        "Answer:\n",
        "\n",
        "###SLR relies on several key assumptions to ensure accurate and reliable results:\n",
        "\n",
        "1. Linearity: There should be a linear relationship between X and Y. Non-linear relationships violate the model’s validity.\n",
        "\n",
        "2. Independence of errors: The residuals (differences between observed and predicted values) should be independent of each other.\n",
        "\n",
        "3. Homoscedasticity: Residuals should have constant variance across all levels of X. Unequal variance (heteroscedasticity) can lead to misleading predictions.\n",
        "\n",
        "4. Normality of residuals: The residuals should follow a normal distribution. This is important for confidence intervals and hypothesis testing.\n",
        "\n",
        "5. No significant outliers: Extreme values can distort the slope and intercept of the regression line, leading to inaccurate predictions.\n",
        "\n",
        "###Example:\n",
        "\n",
        "If you are predicting exam scores based on hours studied, the residuals (errors) should not increase or decrease systematically as hours increase."
      ],
      "metadata": {
        "id": "XtkaiRqs-tmm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 3: Write the mathematical equation for a simple linear regression model and explain each term.\n",
        "Answer:\n",
        "\n",
        "###The mathematical equation of a simple linear regression model is:\n",
        "\n",
        "  # Y = β0 ​+ β1​X + ϵ\n",
        "\n",
        "Where:\n",
        "\n",
        "- Y: Dependent (target) variable we want to predict.\n",
        "\n",
        "- X: Independent (predictor) variable.\n",
        "\n",
        "- β₀ (Intercept): Value of Y when X = 0; it is the point where the regression line crosses the Y-axis.\n",
        "\n",
        "- β₁ (Slope): Change in Y for every one-unit increase in X. It shows the strength and direction of the relationship.\n",
        "\n",
        "- ε (Error term): The difference between the observed value and the predicted value, accounting for randomness or unobserved factors.\n",
        "\n",
        "###Interpretation Example:\n",
        "\n",
        "If β₀ = 2 and β₁ = 3, the equation is Y = 2 + 3X.\n",
        "\n",
        "For every 1-unit increase in X, Y increases by 3 units, and when X = 0, Y = 2"
      ],
      "metadata": {
        "id": "xkAuQI8T-tjO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 4: Provide a real-world example where simple linear regression can be applied.\n",
        "Answer:\n",
        "###Example 1: House Price Prediction\n",
        "\n",
        "- Independent variable (X): Size of the house in square feet.\n",
        "\n",
        "- Dependent variable (Y): Selling price of the house.\n",
        "\n",
        "If the slope β₁ = 5000, it indicates that for every additional square foot, the house price increases by ₹5000. SLR helps real estate analysts estimate pricing trends.\n",
        "\n",
        "###Example 2: Salary Prediction\n",
        "\n",
        "- X: Years of experience\n",
        "\n",
        "- Y: Annual salary\n",
        "\n",
        "SLR can predict expected salary increases based on experience. These examples show that SLR is widely applicable in business, finance, and social sciences.\n",
        "\n",
        "\n",
        "###Other Real-life Examples:\n",
        "\n",
        "  - Predicting crop yield based on rainfall\n",
        "\n",
        "  - Predicting sales based on advertising budget\n"
      ],
      "metadata": {
        "id": "OdqrzqGd-tgp"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZ60pfzNO-kF"
      },
      "source": [
        "## Question 5: What is the method of least squares in linear regression?\n",
        "Answer:\n",
        "\n",
        "The Method of Least Squares is a mathematical approach used to find the best-fitting regression line. It minimizes the sum of the squared differences between actual values (Y) and predicted values ($\\hat{Y}$). This sum of squared differences is often referred to as the Residual Sum of Squares (RSS).\n",
        "\n",
        "- The goal is to find the values of the intercept ($\\beta_0$) and the slope ($\\beta_1$) that minimize this RSS. The formulas for these values can be derived using calculus.\n",
        "\n",
        "Steps:\n",
        "\n",
        "1. **Calculate the predicted value** ($\\hat{Y}$) = $\\beta_0$ + $\\beta_1 X $\n",
        "2. **Compute residuals:** $e_i = Y_i - \\hat{Y}_i = Y_i - (\\beta_0 + \\beta_1 X_i)$.\n",
        "3. **Square the residuals and sum them:**  $\\sum e_i^2$\n",
        "4. Adjust β₀ and β₁ to minimize this sum.\n",
        "\n",
        "#Formula:\n",
        "\n",
        "  $Minimize S(β0​,β1​) = ∑_{i=1}^n (Yi​−(β0​+β1​Xi​))^2$\n",
        "\n",
        "\n",
        "- This ensures the regression line is as close as possible to all data points. Least squares is the foundation of most regression algorithms.\n",
        "\n",
        "- The resulting values for $\\beta_0$ and $\\beta_1$ give the equation of the best-fitting line that minimizes the errors between the predicted and actual values."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 6: What is Logistic Regression? How does it differ from Linear Regression?\n",
        "Answer:\n",
        "\n",
        "**Logistic Regression** is a statistical model used for binary classification problems. Unlike linear regression, which predicts a continuous outcome, logistic regression predicts the probability of a binary outcome (e.g., yes/no, 0/1, true/false). It uses a logistic function (also known as the sigmoid function) to map any real-valued number to a value between 0 and 1, which can then be interpreted as a probability.\n",
        "\n",
        "### $P(Y=1∣X) = \\frac{1}{1+e−(β0​+β1​X)​ }$\n",
        "\n",
        "**Differences from Linear Regression:**\n",
        "\n",
        "  |**Feature**      |**Linear Regression**    | **Logistic Regression**      |\n",
        "  |-----------------|-------------------------|------------------------------|\n",
        "  | Output          | Continuous values       | Probability (0 to 1)         |\n",
        "  | Purpose         | Regression              | Classification               |\n",
        "  | Equation        | Straight line           | Sigmoid curve                |\n",
        "  | Loss function   | MSE                     | Log Loss / Cross-Entropy     |\n",
        "  | Example         | Predicting house price  | Predicting if a student passes fails |\n",
        "\n",
        "\n",
        "**Example:**\n",
        "\n",
        "Predicting whether a patient has diabetes based on age and BMI. Linear regression is unsuitable because probabilities must be between 0 and 1\n",
        "\n",
        "\n",
        "**In summary**, while both are regression techniques, Linear Regression is for predicting continuous values, and Logistic Regression is for predicting probabilities for binary outcomes.\n"
      ],
      "metadata": {
        "id": "eRtuKy6R-tbG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 7: Name and briefly describe three common evaluation metrics for regression models.\n",
        "Answer:\n",
        "\n",
        "1. Mean Absolute Error (MAE):\n",
        " - The average of the absolute differences between the actual values ($Y_i$) and the predicted values ($\\hat{Y}i$). It measures the average magnitude of errors without considering their direction.\n",
        " - Interpretation: Less sensitive to outliers compared to MSE or RMSE. A lower MAE indicates a better model fit.\n",
        "\n",
        "\n",
        " Formula: $MAE = \\frac{1}{n} \\sum{i=1}^n |Y_i - \\hat{Y}_i|$\n",
        "\n",
        "2. Mean Squared Error (MSE):\n",
        "- The average of the squared differences between the actual values ($Y_i$) and the predicted values ($\\hat{Y}_i$). Squaring the errors gives more weight to larger errors.\n",
        "-  Interpretation: Penalizes larger errors more than smaller ones. A lower MSE indicates a better model fit.\n",
        "\n",
        "Formula: $MSE = \\frac{1}{n} \\sum{i=1}^n (Y_i - \\hat{Y}_i)^2$\n",
        "\n",
        "\n",
        "\n",
        "3. Root Mean Squared Error (RMSE):\n",
        "Square root of MSE. Measures error in the same units as Y.\n",
        "\n",
        " - The square root of the Mean Squared Error. It measures the error in the same units as the dependent variable, making it more interpretable than MSE.\n",
        " - Interpretation: Provides a measure of the typical error size in the original units of the dependent variable. A lower RMSE indicates a better model fit.\n",
        "\n",
        "Formula: $RMSE = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2}$\n",
        "\n",
        "In general, for all three metrics, lower values indicate a better-fitting model. The choice of which metric to use can depend on the specific problem and the importance of penalizing larger errors."
      ],
      "metadata": {
        "id": "C4fRBlq3-M83"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUiv1VsAXhGY"
      },
      "source": [
        "## Question 8: What is the purpose of the R-squared metric in regression analysis?\n",
        "Answer:\n",
        "\n",
        "R-squared (R²), also known as the coefficient of determination, is a statistical measure that represents the proportion of the variance in the dependent variable that is predictable from the independent variable(s) in a regression model.\n",
        "\n",
        "In simpler terms, R-squared tells you how well the regression model fits the observed data. It indicates how much of the variation in the dependent variable can be explained by the independent variable(s).\n",
        "\n",
        "The formula for R-squared is:\n",
        "\n",
        "$R^2 = 1 - \\frac{SSE}{SST}$\n",
        "\n",
        "Where:\n",
        "\n",
        "*   **SSE (Sum of Squared Errors or Residual Sum of Squares):** The sum of the squared differences between the actual values and the predicted values from the regression model. It represents the unexplained variance.\n",
        "*   **SST (Total Sum of Squares):** The sum of the squared differences between the actual values and the mean of the dependent variable. It represents the total variance in the dependent variable.\n",
        "\n",
        "Interpretation:\n",
        "\n",
        "*   An R² value of 1 means that the model perfectly explains all the variability in the dependent variable.\n",
        "*   An R² value of 0 means that the model explains none of the variability in the dependent variable.\n",
        "*   Higher R² values generally indicate a better-fitting model, as they suggest that a larger proportion of the variance in the dependent variable is explained by the independent variable(s).\n",
        "\n",
        "However, it's important to note that a high R² does not necessarily mean the model is good or that the independent variable(s) are the cause of the changes in the dependent variable. It's just a measure of how well the model fits the data. It's also important to consider other evaluation metrics and the context of the problem."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 9: Write Python code to fit a simple linear regression model using scikit-learn and print the slope and intercept.\n",
        "\n",
        "Answer:"
      ],
      "metadata": {
        "id": "GiiShQrv-NSh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "import numpy as np\n",
        "\n",
        "# Sample data\n",
        "X = np.array([[1], [2], [3], [4], [5]])      # Independent variable\n",
        "y = np.array([2, 4, 5, 4, 5])                # Dependent variable\n",
        "\n",
        "# Create model\n",
        "model = LinearRegression()\n",
        "\n",
        "# Train model\n",
        "model.fit(X, y)\n",
        "\n",
        "# Print slope and intercept\n",
        "print(\"Slope (β1):\", model.coef_[0])\n",
        "print(\"Intercept (β0):\", model.intercept_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EhsdzmI1YVOU",
        "outputId": "a84442a8-cdb0-43aa-a126-5f8d723f102f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Slope (β1): 0.6\n",
            "Intercept (β0): 2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FwrkR8q9ZgRr"
      },
      "source": [
        "## Question 10: How do you interpret the coefficients in a simple linear regression model?\n",
        "Answer:\n",
        "\n",
        "In a simple linear regression model, the equation is typically represented as $Y = \\beta_0 + \\beta_1 X + \\epsilon$, where:\n",
        "\n",
        "*   **$\\beta_0$ (Intercept):** This is the predicted value of the dependent variable (Y) when the independent variable (X) is equal to 0. It represents the point where the regression line crosses the Y-axis. However, interpreting the intercept only makes sense if X=0 is a meaningful value in the context of your data.\n",
        "\n",
        "*   **$\\beta_1$ (Slope):** This coefficient represents the change in the dependent variable (Y) for every one-unit increase in the independent variable (X).\n",
        "    *   A **positive slope** ($\\beta_1 > 0$) indicates that as X increases, Y also tends to increase.\n",
        "    *   A **negative slope** ($\\beta_1 < 0$) indicates that as X increases, Y tends to decrease.\n",
        "    *   The magnitude of the slope indicates the strength of the linear relationship between X and Y. A larger absolute value of $\\beta_1$ suggests a stronger relationship.\n",
        "\n",
        "**Example:**\n",
        "\n",
        "Consider a simple linear regression model predicting house price (Y) based on the size of the house in square feet (X), with the equation: `House Price = β₀ + β₁ * Size`.\n",
        "\n",
        "*   If $\\beta_0 = 50000$ and $\\beta_1 = 100$, the interpretation would be:\n",
        "    *   The intercept ($\\beta_0 = 50000$) would suggest that a house with 0 square feet is predicted to cost $50,000. This interpretation is likely not meaningful in this context, as a house cannot have 0 square feet.\n",
        "    *   The slope ($\\beta_1 = 100$) indicates that for every additional square foot of size, the predicted house price increases by $100.\n",
        "\n",
        "Understanding the interpretation of these coefficients is crucial for understanding the relationship between the variables in your model and for making predictions."
      ]
    }
  ]
}